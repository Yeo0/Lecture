{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task 5\n",
    "\n",
    "Create an automated way, how to scrap wikipedia pages. The scrapper output for given wikipedia page should be HTML file containing abstracts to all linked wikipedia pages.\n",
    "\n",
    "## Every abstract should contain:\n",
    "1. Heading - name of wikipedia linked page\n",
    "2. Link to the wikipedia page\n",
    "3. The first paragraph of the wikipedia page\n",
    "\n",
    "## Notes:\n",
    "- ignore external links and links to images\n",
    "\n",
    "## Example:\n",
    "For wikipedia page <a href=\"https://en.wikipedia.org/wiki/Architecture_of_Seattle\">\"https://en.wikipedia.org/wiki/Architecture_of_Seattle\"</a> the output is displayed in file `data/task5_output.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progressing... (0/131)\n",
      "Progressing... (1/131)\n",
      "Progressing... (2/131)\n",
      "Progressing... (3/131)\n",
      "Progressing... (4/131)\n",
      "Progressing... (5/131)\n",
      "Progressing... (6/131)\n",
      "Progressing... (7/131)\n",
      "Progressing... (8/131)\n",
      "Progressing... (9/131)\n",
      "Progressing... (10/131)\n",
      "Progressing... (11/131)\n",
      "Progressing... (12/131)\n",
      "Progressing... (13/131)\n",
      "Progressing... (14/131)\n",
      "Progressing... (15/131)\n",
      "Progressing... (16/131)\n",
      "Progressing... (17/131)\n",
      "Progressing... (18/131)\n",
      "Progressing... (19/131)\n",
      "Progressing... (20/131)\n",
      "Progressing... (21/131)\n",
      "Progressing... (22/131)\n",
      "Progressing... (23/131)\n",
      "Progressing... (24/131)\n",
      "Progressing... (25/131)\n",
      "Progressing... (26/131)\n",
      "Progressing... (27/131)\n",
      "Progressing... (28/131)\n",
      "Progressing... (29/131)\n",
      "Progressing... (30/131)\n",
      "Progressing... (31/131)\n",
      "Progressing... (32/131)\n",
      "Progressing... (33/131)\n",
      "Progressing... (34/131)\n",
      "Progressing... (35/131)\n",
      "Progressing... (36/131)\n",
      "Progressing... (37/131)\n",
      "Progressing... (38/131)\n",
      "Progressing... (39/131)\n",
      "Progressing... (40/131)\n",
      "Progressing... (41/131)\n",
      "Progressing... (42/131)\n",
      "Progressing... (43/131)\n",
      "Progressing... (44/131)\n",
      "Progressing... (45/131)\n",
      "Progressing... (46/131)\n",
      "Progressing... (47/131)\n",
      "Progressing... (48/131)\n",
      "Progressing... (49/131)\n",
      "Progressing... (50/131)\n",
      "Progressing... (51/131)\n",
      "Progressing... (52/131)\n",
      "Progressing... (53/131)\n",
      "Progressing... (54/131)\n",
      "Progressing... (55/131)\n",
      "Progressing... (56/131)\n",
      "Progressing... (57/131)\n",
      "Progressing... (58/131)\n",
      "Progressing... (59/131)\n",
      "Progressing... (60/131)\n",
      "Progressing... (61/131)\n",
      "Progressing... (62/131)\n",
      "Progressing... (63/131)\n",
      "Progressing... (64/131)\n",
      "Progressing... (65/131)\n",
      "Progressing... (66/131)\n",
      "Progressing... (67/131)\n",
      "Progressing... (68/131)\n",
      "Progressing... (69/131)\n",
      "Progressing... (70/131)\n",
      "Progressing... (71/131)\n",
      "Progressing... (72/131)\n",
      "Progressing... (73/131)\n",
      "Progressing... (74/131)\n",
      "Progressing... (75/131)\n",
      "Progressing... (76/131)\n",
      "Progressing... (77/131)\n",
      "Progressing... (78/131)\n",
      "Progressing... (79/131)\n",
      "Progressing... (80/131)\n",
      "Progressing... (81/131)\n",
      "Progressing... (82/131)\n",
      "Progressing... (83/131)\n",
      "Progressing... (84/131)\n",
      "Progressing... (85/131)\n",
      "Progressing... (86/131)\n",
      "Progressing... (87/131)\n",
      "Progressing... (88/131)\n",
      "Progressing... (89/131)\n",
      "Progressing... (90/131)\n",
      "Progressing... (91/131)\n",
      "Progressing... (92/131)\n",
      "Progressing... (93/131)\n",
      "Progressing... (94/131)\n",
      "Progressing... (95/131)\n",
      "Progressing... (96/131)\n",
      "Progressing... (97/131)\n",
      "Progressing... (98/131)\n",
      "Progressing... (99/131)\n",
      "Progressing... (100/131)\n",
      "Progressing... (101/131)\n",
      "Progressing... (102/131)\n",
      "Progressing... (103/131)\n",
      "Progressing... (104/131)\n",
      "Progressing... (105/131)\n",
      "Progressing... (106/131)\n",
      "Progressing... (107/131)\n",
      "Progressing... (108/131)\n",
      "Progressing... (109/131)\n",
      "Progressing... (110/131)\n",
      "Progressing... (111/131)\n",
      "Progressing... (112/131)\n",
      "Progressing... (113/131)\n",
      "Progressing... (114/131)\n",
      "Progressing... (115/131)\n",
      "Progressing... (116/131)\n",
      "Progressing... (117/131)\n",
      "Progressing... (118/131)\n",
      "Progressing... (119/131)\n",
      "Progressing... (120/131)\n",
      "Progressing... (121/131)\n",
      "Progressing... (122/131)\n",
      "Progressing... (123/131)\n",
      "Progressing... (124/131)\n",
      "Progressing... (125/131)\n",
      "Progressing... (126/131)\n",
      "Progressing... (127/131)\n",
      "Progressing... (128/131)\n",
      "Progressing... (129/131)\n",
      "Progressing... (130/131)\n",
      "Progressing... (131/131)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "f = open('./take_5_output.html','w')\n",
    "message = \"<html><head></head><body>\"\n",
    "main_domain = 'https://en.wikipedia.org'\n",
    "url = 'https://en.wikipedia.org/wiki/Brainpool_TV'\n",
    "r = requests.get(url)\n",
    "sample_html = r.text\n",
    "soup = BeautifulSoup(sample_html, \"html.parser\")\n",
    "\n",
    "a_tags = soup.find_all(\"a\")\n",
    "\n",
    "index = 0\n",
    "for a in a_tags:\n",
    "    img_tag = a.find_all(\"img\")\n",
    "    if len(img_tag) == 0:\n",
    "        href = a.get('href')\n",
    "        if href != None:\n",
    "            if len(href.split('/'))>2 and len(href.split(':'))==1 and href.split('/')[1] == 'wiki':\n",
    "                new_url = main_domain + a.get('href')\n",
    "                resource = requests.get(url)\n",
    "                new_page_html = resource.text\n",
    "                new_page_soup = BeautifulSoup(new_page_html, \"html.parser\")\n",
    "                p_tags = new_page_soup.find_all(\"p\")\n",
    "                if len(p_tags) > 0:\n",
    "                    tag = \"<h1>\"+a.getText()+\"</h1><a href='\" + new_url + \"'>\"+a.getText()+\"</a><p>\"+p_tags[0].getText()+\"</p>\"\n",
    "                    message = message + tag\n",
    "    \n",
    "    print(\"Progressing... (%d/%d)\"%(index, len(a_tags)-1))\n",
    "    index = index + 1\n",
    "    \n",
    "message = message + \"</body></html>\"\n",
    "message = message.encode('utf-8')\n",
    "\n",
    "f.write(str(message))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
